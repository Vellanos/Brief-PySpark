{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e3d25e",
   "metadata": {},
   "source": [
    "## Partie 2 : Implémenter pas à pas le pipeline\n",
    "\n",
    "Écrire un script Python qui :\n",
    "\n",
    "- charge les données d’entrée dans des DataFrames pandas (CSV/JSON/…),\n",
    "- applique les transformations définies,\n",
    "- sauvegarde le résultat dans un fichier CSV et dans une base de données conformément au cahier - des charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7777c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json, sqlite3\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad3b1b",
   "metadata": {},
   "source": [
    "### Configuration (settings.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a591651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_settings(path=\"settings.yaml\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c636c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_settings()\n",
    "in_dir = cfg.get(\"input_dir\", \"./data/march-input\")\n",
    "out_dir = cfg.get(\"output_dir\", \"./data/out\")\n",
    "db_path = cfg.get(\"db_path\", \"./data/sales_db.db\")\n",
    "sep = cfg.get(\"csv_sep\",\";\")\n",
    "enc = cfg.get(\"csv_encoding\",\"utf-8\")\n",
    "ffmt = cfg.get(\"csv_float_format\",\"%.2f\")\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "display(Markdown(\n",
    "    f\"Input dir: `{in_dir}`\\n\"\n",
    "    f\"Output dir: `{out_dir}`\\n\"\n",
    "    f\"DB (SQLite) : `{db_path}`\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac37958f",
   "metadata": {},
   "source": [
    "### 1- Charger les données d’entrée dans des DataFrames pandas (CSV/JSON/…)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07252d1e",
   "metadata": {},
   "source": [
    "Charger les informations sur les clients `customers.csv`\n",
    "\n",
    "Syntaxe commande : `out_df = pd.read_csv(\"input path\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab23315",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_path = os.path.join(in_dir, \"customers.csv\")\n",
    "#customers_path = (\"/chemin vers /customers.csv\")\n",
    "if not os.path.exists(customers_path):\n",
    "    display(Markdown(f\" Fichier manquant : `{customers_path}`.\"))\n",
    "else:\n",
    "    customers = pd.read_csv(customers_path)\n",
    "    display(customers.head(30))\n",
    "    display(Markdown(f\"Taille: {customers.shape}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982dd8f3",
   "metadata": {},
   "source": [
    "Charger les informations sur les clients `refunds.csv`\n",
    "\n",
    "Syntaxe commande : `out_df = pd.read_csv(\"input path\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refunds_path = os.path.join(in_dir, \"refunds.csv\")\n",
    "#refunds_path = (\"/chemin vers /refunds.csv\")\n",
    "if not os.path.exists(refunds_path):\n",
    "    display(Markdown(f\" Fichier manquant : `{refunds_path}`.\"))\n",
    "else:\n",
    "    refunds = pd.read_csv(refunds_path)\n",
    "    display(refunds.head())\n",
    "    display(Markdown(f\"Taille: {refunds.shape}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4654d",
   "metadata": {},
   "source": [
    "Charger les informations sur les commandes `orders_*.json`\n",
    "\n",
    "Syntaxe commande : `df = pd.read_json(\"data.json\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2fae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_path = os.path.join(in_dir, \"orders_2025-03-01.json\")\n",
    "#order_path = (\"/Users/estherchabi/Documents/fr/Caplogy/2025-2026/Simplon/Inter UI ARA HDF /Esther_Phase0/Brief1111/projet_python/data/march-input/orders_2025-03-01.json\")\n",
    "\n",
    "if not os.path.exists(order_path):\n",
    "    display(Markdown(f\" Fichier manquant : `{order_path}`.\"))\n",
    "else:\n",
    "    order = pd.read_json(order_path)\n",
    "    display(order.head())\n",
    "    display(Markdown(f\"Taille: {order.shape}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c308ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#day=1\n",
    "#order_path = os.path.join(in_dir, f\"orders_2025-03-{day:02d}.json\")\n",
    "#display(Markdown(f\"Order dir: `{order_path}`\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dbe175",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = []\n",
    "\n",
    "for day in range(1, 32): \n",
    "    order_path = os.path.join(in_dir, f\"orders_2025-03-{day:02d}.json\")\n",
    "\n",
    "    if not os.path.exists(order_path):\n",
    "        display(Markdown(f\" Fichier manquant : `{order_path}`.\"))\n",
    "        continue\n",
    "    else:\n",
    "        order = pd.read_json(order_path)\n",
    "        #display(Markdown(f\"Taille: {order.shape}\")) # Taille: (103, 6)\n",
    "\n",
    "    liste.append(order)\n",
    "    #display(Markdown(f\"Taille : {liste}\"))\n",
    "\n",
    "orders = pd.concat(liste) \n",
    "\n",
    "display(orders.head())\n",
    "display(Markdown(f\"Taille: {orders.shape}\")) # Taille: (103x31, 6) = (3193, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462ff8d",
   "metadata": {},
   "source": [
    "### Appliquer les transformations définies dans le cahier de charge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c6339",
   "metadata": {},
   "source": [
    "#### Transformation client:\n",
    "- Exclure les clients inactifs (is_active = false)\n",
    "\n",
    "La colonne is_active doit être booléenne (j'ai écris une petite fonction qui vérifie que la colonne est booleenne)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f61302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def controle_bool(v):\n",
    "    if isinstance(v, bool): return v\n",
    "    if isinstance(v, (int, float)): return bool(v)\n",
    "    if v is None: return False\n",
    "    s = str(v).strip().lower()\n",
    "    return s in (\"1\",\"true\",\"yes\",\"y\",\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aeaee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers[\"is_active\"] = customers[\"is_active\"].apply(controle_bool)\n",
    "customers = customers.astype({\"customer_id\":\"string\",\"city\":\"string\"})\n",
    "display(Markdown(\"Affichage clients (après nettoyage)\"))\n",
    "display(customers.head(30))\n",
    "display(Markdown(f\"Taille: {customers.shape}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e5a23",
   "metadata": {},
   "source": [
    "#### Transformation remboursements (refunds)\n",
    "\n",
    "Pas de transformations précisées explicitement dans le cachier de charge, par contre on me demande de : agréger les remboursements par commande. Il faudra donc que je m'assure que la colonne `amount` contient des montants numériques décimaux propres, afin de permettre une agrégation fiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ae6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refunds = pd.read_csv(refunds_path)\n",
    "refunds[\"amount\"] = pd.to_numeric(refunds[\"amount\"], errors=\"coerce\").fillna(0.0)\n",
    "refunds[\"created_at\"] = refunds[\"created_at\"].astype(\"string\")\n",
    "display(Markdown(\"Aperçu remboursements (après coercition numérique)\"))\n",
    "display(refunds.head())\n",
    "display(Markdown(f\"Taille: {refunds.shape}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb1e69",
   "metadata": {},
   "source": [
    "### Transformation commandes : orders\n",
    "\n",
    "\n",
    "- Conserver uniquement les commandes payées (payment_status = 'paid')\n",
    "- Écarter toute ligne d’article avec prix unitaire négatif (et consigner\n",
    "ces rejets)\n",
    "- Dédupliquer sur order_id (garder la première occurrence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6837245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrer les commandes payées (payment_status == 'paid')\n",
    "# On peut aussi ecrire une fonction qui verifie que le status est bien 'paid'\n",
    "# ex: p,ok,yes => paid\n",
    "# def controle_paid(v):\n",
    "#     v = v.lower()\n",
    "#     if v in ['p', 'ok', 'yes']:\n",
    "#         return 'paid'\n",
    "#     return 'other'   \n",
    "# orders[\"payment_status\"] = orders[\"payment_status\"].apply(controle_paid) \n",
    "ln_initial = len(orders)\n",
    "orders = orders[orders[\"payment_status\"]==\"paid\"].copy()\n",
    "ln_final = len(orders)\n",
    "display(Markdown(f\"Filtrage payées : {ln_initial} → {ln_final}\"))\n",
    "display(orders.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756020d0",
   "metadata": {},
   "source": [
    "Ecarter toute ligne d’article dont le prix unitaire est négatif et journaliser ces rejets.\n",
    "\n",
    "Le prix unitaire est porté par la structure items : il faut d’abord aplatir/“exploser” cette colonne pour obtenir une ligne par article, puis appliquer le filtre.\n",
    "Aplatissement possible via `pandas.json_normalize` :\n",
    "items = pd.json_normalize(orders[\"items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders2 = orders\n",
    "display(Markdown(\"Avant explosion des items\"))\n",
    "display(orders2.head())\n",
    "#display(Markdown(f\"Colonnes: {list(orders2.columns)[:12]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders2 = orders2.explode(\"items\", ignore_index=True)\n",
    "display(orders.head())\n",
    "items = pd.json_normalize(orders2[\"items\"]).add_prefix(\"item_\")\n",
    "display(items)\n",
    "orders2 = pd.concat([orders2.drop(columns=[\"items\"]), items], axis=1)\n",
    "display(Markdown(\"Après explosion des items\"))\n",
    "display(orders2.head())\n",
    "display(Markdown(f\"Colonnes: {list(orders2.columns)[:12]} ...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_mask = orders2[\"item_unit_price\"] < 0\n",
    "n_neg = int(neg_mask.sum())\n",
    "display(Markdown(f\"Lignes prix négatifs : {n_neg}\"))\n",
    "if n_neg > 0:\n",
    "    rejects_items = orders2.loc[neg_mask].copy()\n",
    "    rejects_path = os.path.join(out_dir, \"rejects_items.csv\")\n",
    "    rejects_items.to_csv(rejects_path, index=False, encoding=enc)\n",
    "    display(Markdown(f\" Rejets sauvegardés : `{rejects_path}`\"))\n",
    "    orders2 = orders2.loc[~neg_mask].copy()\n",
    "display(orders2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787721e",
   "metadata": {},
   "source": [
    "Dédupliquer sur order_id (garder la première occurrence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = len(orders2)\n",
    "orders3 = orders2.sort_values([\"order_id\",\"created_at\"]).drop_duplicates(subset=[\"order_id\"], keep=\"first\")\n",
    "after = len(orders3)\n",
    "display(Markdown(f\"Déduplication : **{before} → {after}**\"))\n",
    "display(orders3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd0980",
   "metadata": {},
   "source": [
    "### Aggregations et jointures\n",
    "\n",
    "Sortie attendue : \n",
    "Colonnes: date;city;`channel`;orders_count;unique_customers;`items_sold`;`gross_revenue_eur`;refunds_eur;net_revenue_eur\n",
    "\n",
    "On peut commencer avec la table oders\n",
    "- Calculer une colonne line_gross = item_qty x item_unit_price\n",
    "- Agreger les données selon (order_id, customer_id, channel, created_at) en totalisant items_sold et gross_revenue_eur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders3[\"line_gross\"] = orders3[\"item_qty\"] * orders3[\"item_unit_price\"]\n",
    "per_order = orders3.groupby([\"order_id\",\"customer_id\",\"channel\",\"created_at\"], as_index=False).agg(\n",
    "    items_sold=(\"item_qty\",\"sum\"),\n",
    "    gross_revenue_eur=(\"line_gross\",\"sum\")\n",
    ")\n",
    "display(Markdown(\"Aperçu `per_order`\"))\n",
    "display(per_order.head())\n",
    "display(Markdown(f\"Taille: {per_order.shape}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed35088",
   "metadata": {},
   "source": [
    " Jointure avec la table clients\n",
    " \n",
    " Colonnes: date;`city`;`channel`;orders_count;unique_customers;`items_sold`;`gross_revenue_eur`;refunds_eur;net_revenue_eur\n",
    "\n",
    "\n",
    "Ici on réalise une jointure entre la table des commandes per_order et la table customers afin d’enrichir chaque commande avec les informations client (city, is_active) à partir de la clé commune customer_id. \n",
    "\n",
    "Le paramètre how=\"left\" indique qu’il s’agit d’une jointure gauche : on garde toutes les lignes de per_order et on ajoute les données de customers quand un customer_id correspondant existe (sinon on obtient des valeurs nulles). \n",
    "\n",
    "En pandas (comme en SQL), on dispose aussi d’autres types de jointures :\n",
    "- inner : ne conserve que les lignes présentes dans les deux tables (intersection),\n",
    "- right : symétrique de left, on garde toutes les lignes de la table de droite,\n",
    "- outer : conserve toutes les lignes des deux tables, en complétant avec des valeurs nulles lorsqu’il manque une correspondance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eda258",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_init = len(per_order)\n",
    "per_order = per_order.merge(customers[[\"customer_id\",\"city\",\"is_active\"]], on=\"customer_id\", how=\"left\")\n",
    "per_order = per_order[per_order[\"is_active\"]==True].copy() # Important pour respecter le cahier de charge\n",
    "ln_aft = len(per_order)\n",
    "display(Markdown(f\"Après jointure+filtre actifs : **{len_init} → {ln_aft}**\"))\n",
    "display(per_order.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fba3a1",
   "metadata": {},
   "source": [
    "Extraire la **date** depuis `created_at`\n",
    "\n",
    "La fonction `to_date()` renvoie la date au format texte \"YYYY-MM-DD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(s):\n",
    "    s = str(s)\n",
    "    for fmt in (\"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt).date().isoformat()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(f\"Format de date non reconnu: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_order[\"order_date\"] = per_order[\"created_at\"].apply(to_date)\n",
    "display(per_order[[\"order_id\",\"created_at\",\"order_date\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6d745",
   "metadata": {},
   "source": [
    " Jointure avec la table refunds\n",
    " \n",
    " Colonnes: date;`city`;`channel`;orders_count;unique_customers;`items_sold`;`gross_revenue_eur`;`refunds_eur`;net_revenue_eur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0678c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "refunds_sum = refunds.groupby(\"order_id\", as_index=False)[\"amount\"].sum().rename(columns={\"amount\":\"refunds_eur\"}) # Somme des remboursements par order_id (commande)\n",
    "per_order = per_order.merge(refunds_sum, on=\"order_id\", how=\"left\").fillna({\"refunds_eur\":0.0})\n",
    "display(per_order.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b7539",
   "metadata": {},
   "source": [
    "### Sauvegarde intermédiaire dans SQLite (`orders_clean`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "per_order_save = per_order[[\"order_id\",\"customer_id\",\"city\",\"channel\",\"order_date\",\"items_sold\",\"gross_revenue_eur\"]].copy()\n",
    "per_order_save.to_sql(\"orders_clean\", conn, if_exists=\"replace\", index=False)\n",
    "display(Markdown(\"Table `orders_clean` sauvegardée dans SQLite\"))\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef47775",
   "metadata": {},
   "source": [
    "### Agrégat final (date, ville, canal) + **net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = per_order.groupby([\"order_date\",\"city\",\"channel\"], as_index=False).agg(\n",
    "    orders_count=(\"order_id\",\"nunique\"),\n",
    "    unique_customers=(\"customer_id\",\"nunique\"),\n",
    "    items_sold=(\"items_sold\",\"sum\"),\n",
    "    gross_revenue_eur=(\"gross_revenue_eur\",\"sum\"),\n",
    "    refunds_eur=(\"refunds_eur\",\"sum\")\n",
    ")\n",
    "agg[\"net_revenue_eur\"] = agg[\"gross_revenue_eur\"] + agg[\"refunds_eur\"]\n",
    "agg = agg.rename(columns={\"order_date\":\"date\"}).sort_values([\"date\",\"city\",\"channel\"]).reset_index(drop=True)\n",
    "display(agg.head())\n",
    "display(Markdown(f\"Taille: {agg.shape}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6d2ca",
   "metadata": {},
   "source": [
    "### Écriture finale en base (`daily_city_sales`) + exports CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bffdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "agg.to_sql(\"daily_city_sales\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()\n",
    "display(Markdown(\"Table `daily_city_sales` écrite dans SQLite\"))\n",
    "# Exports CSV\n",
    "for d, sub in agg.groupby(\"date\"):\n",
    "    out_path = os.path.join(out_dir, f\"daily_summary_{d.replace('-','')}.csv\")\n",
    "    sub[[\n",
    "        \"date\",\"city\",\"channel\",\"orders_count\",\"unique_customers\",\"items_sold\",\n",
    "        \"gross_revenue_eur\",\"refunds_eur\",\"net_revenue_eur\"\n",
    "    ]].to_csv(out_path, index=False, sep=sep, encoding=enc, float_format=ffmt)\n",
    "all_path = os.path.join(out_dir, \"daily_summary_all.csv\")\n",
    "agg.to_csv(all_path, index=False, sep=sep, encoding=enc, float_format=ffmt)\n",
    "display(Markdown(f\"Exports CSV dans `{out_dir}`\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brief-pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
