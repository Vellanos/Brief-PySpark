{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a5fee1",
   "metadata": {},
   "source": [
    "# Introduction aux RDD avec PySpark\n",
    "\n",
    "Ce notebook présente les **RDD (Resilient Distributed Datasets)** dans Apache Spark, en version **PySpark**.\n",
    "\n",
    "Objectifs :\n",
    "- Comprendre ce qu’est un **RDD** et ses propriétés (résilient, distribué, immuable, lazy).\n",
    "- Savoir **créer** des RDD (à partir de collections locales, de fichiers texte).\n",
    "- Utiliser les **transformations** (`map`, `filter`, `flatMap`, `reduceByKey`, …).\n",
    "- Utiliser les **actions** (`collect`, `count`, `reduce`, `take`, …).\n",
    "- Comprendre la **persistance** (`cache`, `persist`).\n",
    "- Construire un **exemple complet** : *Word Count*.\n",
    "\n",
    "\n",
    "Prérequis :\n",
    "- Python installé\n",
    "- PySpark installé \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4f72ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/19 14:42:20 WARN Utils: Your hostname, DavidSimplon, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/19 14:42:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/19 14:42:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7699e9044e90>\n",
      "<SparkContext master=local[*] appName=TutoRDD_PySpark>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Création d'une SparkSession locale\n",
    "spark = SparkSession.builder.appName(\"TutoRDD_PySpark\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Récupération du SparkContext (point d'entrée RDD)\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35a84a",
   "metadata": {},
   "source": [
    "## 1. Qu’est-ce qu’un RDD ?\n",
    "\n",
    "Un **RDD (Resilient Distributed Dataset)** est la **structure de données de base** de Spark.\n",
    "\n",
    "Un RDD est :\n",
    "\n",
    "- **Resilient (tolérant aux pannes)**  \n",
    "  - Spark peut **recalculer** les partitions perdues à partir de l’historique des transformations (*lineage*).\n",
    "  - Cet historique est représenté par un **DAG (Directed Acyclic Graph)**.\n",
    "\n",
    "- **Distributed (distribué)**  \n",
    "  - Les données sont **réparties en partitions** sur plusieurs nœuds du cluster.\n",
    "  - Les calculs sont exécutés **en parallèle** sur ces partitions.\n",
    "\n",
    "- **Immutable (immuable)**  \n",
    "  - Un RDD ne peut pas être modifié une fois créé.\n",
    "  - Chaque transformation (ex : `map`, `filter`) **crée un nouveau RDD**.\n",
    "\n",
    "- **Lazy (évaluation paresseuse)**  \n",
    "  - Les transformations ne sont **pas exécutées immédiatement**.\n",
    "  - Spark construit un **plan d’exécution (DAG)**.\n",
    "  - Le calcul n’est déclenché que lorsqu’on appelle une **action** (`collect`, `count`, etc.).\n",
    "\n",
    "En résumé :  \n",
    "> Un RDD est une **abstraction de données distribuées**, immuables et tolérantes aux pannes, manipulées via des **transformations** et des **actions**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b6783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un RDD à partir d'une collection locale Python\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"Type de rdd :\", type(rdd))\n",
    "print(\"Contenu du RDD :\", rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611ffdb",
   "metadata": {},
   "source": [
    "## 2. Créer des RDD\n",
    "\n",
    "Deux méthodes classiques pour créer des RDD :\n",
    "\n",
    "1. **`sc.parallelize(seq)`**  \n",
    "   À partir d’une **collection locale** (liste Python, etc.).  \n",
    "   Utile pour des petits exemples, tests, prototypage.\n",
    "\n",
    "2. **`sc.textFile(path)`**  \n",
    "   À partir d’un **fichier texte** (local, HDFS, S3, …).  \n",
    "   Retourne un **RDD de chaînes** (`RDD[str]`), une ligne par élément.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f10136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple avec parallelize\n",
    "data = list(range(1, 6))\n",
    "rdd_numbers = sc.parallelize(data)\n",
    "\n",
    "print(\"RDD numbers :\", rdd_numbers.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture d'un fichier texte pour créer un RDD\n",
    "rdd_lines = sc.textFile(\"data.txt\")\n",
    "\n",
    "print(\"Nombre de lignes dans data.txt :\", rdd_lines.count())\n",
    "print(\"Quelques lignes :\")\n",
    "for line in rdd_lines.take(5):\n",
    "    print(\"->\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac6560",
   "metadata": {},
   "source": [
    "## 3. Transformations et actions sur les RDD\n",
    "\n",
    "Les opérations sur les RDD se classent en deux catégories :\n",
    "\n",
    "### 3.1 Transformations\n",
    "\n",
    "- **Créent un nouveau RDD** à partir d’un RDD existant.\n",
    "- **Évaluation paresseuse** : elles ne sont **pas exécutées tout de suite**.\n",
    "- Exemples : `map`, `filter`, `flatMap`, `distinct`, `union`, `intersection`,  \n",
    "  `groupBy`, `groupByKey`, `reduceByKey`, `sortBy`, …\n",
    "\n",
    "### 3.2 Actions\n",
    "\n",
    "- **Déclenchent réellement l’exécution** des transformations.\n",
    "- Renvoient un **résultat au driver** ou écrivent des données.\n",
    "- Exemples : `collect`, `count`, `take`, `reduce`, `saveAsTextFile`, …\n",
    "\n",
    "> Les **transformations décrivent le “quoi faire”**,  \n",
    "> les **actions déclenchent le “fais-le maintenant”**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_base = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Deux transformations (lazy) : rien n'est exécuté ici\n",
    "rdd_transformed = rdd_base.map(lambda x: x * 2).filter(lambda x: x > 5)\n",
    "\n",
    "# C'est seulement ici, avec une ACTION, que Spark exécute le plan\n",
    "result = rdd_transformed.collect()\n",
    "print(result)  # [6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8bf3c",
   "metadata": {},
   "source": [
    "## 4. Transformations courantes\n",
    "\n",
    "### 4.1 `map` – Transformer chaque élément\n",
    "\n",
    "**Description :** applique une fonction à chaque élément du RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf438b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1, 6))\n",
    "doubled = rdd.map(lambda x: x * 2)\n",
    "\n",
    "print(doubled.collect())  # [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54daa96",
   "metadata": {},
   "source": [
    "### 4.2 `filter` – Garder certains éléments\n",
    "\n",
    "**Description :** ne conserve que les éléments qui vérifient un prédicat.\n",
    "\n",
    "Exemple : garder les nombres pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f955cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1, 6))\n",
    "even = rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "print(even.collect())  # nombres pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ec320",
   "metadata": {},
   "source": [
    "### 4.3 `flatMap` – Transformer et aplatir\n",
    "\n",
    "**Description :** comme `map`, mais la fonction retourne une **séquence**,  \n",
    "et Spark **aplatit** toutes les séquences en un seul RDD.\n",
    "\n",
    "Exemple : découper des phrases en mots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc10b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"bonjour scala\", \"hello spark\"])\n",
    "\n",
    "words = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "print(words.collect())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9747d10",
   "metadata": {},
   "source": [
    "### 4.4 `distinct` – Supprimer les doublons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 2, 3, 4, 4])\n",
    "unique = rdd.distinct()\n",
    "\n",
    "print(unique.collect())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc5146",
   "metadata": {},
   "source": [
    "### 4.5 `union` – Fusionner deux RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca732233",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([4, 5, 6])\n",
    "\n",
    "combined = rdd1.union(rdd2)\n",
    "\n",
    "print(combined.collect())  # [1, 2, 3, 4, 5, 6] (ordre non garanti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda098d",
   "metadata": {},
   "source": [
    "### 4.6 `intersection` – Obtenir les éléments communs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(range(1, 6))\n",
    "rdd2 = sc.parallelize(range(3, 8))\n",
    "\n",
    "inter = rdd1.intersection(rdd2)\n",
    "\n",
    "print(sorted(inter.collect())) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6fedc",
   "metadata": {},
   "source": [
    "### 4.7 `groupBy` – Grouper selon une clé personnalisée\n",
    "\n",
    "Ici, on sépare les nombres pairs et impairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1, 6))\n",
    "\n",
    "grouped = rdd.groupBy(lambda x: x % 2)\n",
    "\n",
    "# groupBy retourne (clé, iterable)\n",
    "for key, values in grouped.collect():\n",
    "    print(key, list(values))\n",
    "# Ex : 0 [2, 4]  et 1 [1, 3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e365f3",
   "metadata": {},
   "source": [
    "### 4.8 `groupByKey` et `reduceByKey` (RDD de paires)\n",
    "\n",
    "On travaille ici avec un RDD de **paires (clé, valeur)**, par exemple `(mot, 1)`.\n",
    "\n",
    "- `groupByKey` : regroupe **toutes les valeurs** par clé → peut être **très coûteux** (gros shuffle).\n",
    "- `reduceByKey` : agrège les valeurs par clé **au fur et à mesure**, beaucoup plus efficace.\n",
    "\n",
    "**Bonne pratique** : préférer `reduceByKey` quand c’est possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"a\", 1), (\"b\", 1), (\"a\", 2)]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "grouped = rdd.groupByKey()\n",
    "\n",
    "for key, values in grouped.collect():\n",
    "    print(key, list(values))\n",
    "# ('a', [1, 2]), ('b', [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a796d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"a\", 1), (\"b\", 1), (\"a\", 2)]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "reduced = rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(reduced.collect())  # [('a', 3), ('b', 1)] (ordre non garanti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba3eba1",
   "metadata": {},
   "source": [
    "### 4.9 `sortBy` – Trier un RDD\n",
    "\n",
    "On peut trier selon une fonction clé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([5, 2, 8, 1])\n",
    "\n",
    "sorted_rdd = rdd.sortBy(lambda x: x)\n",
    "\n",
    "print(sorted_rdd.collect())  # [1, 2, 5, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4bd575",
   "metadata": {},
   "source": [
    "## 5. Actions sur les RDD\n",
    "\n",
    "Les **actions** déclenchent l’exécution du plan de calcul et retournent un résultat ou écrivent des données.\n",
    "\n",
    "Actions courantes :\n",
    "\n",
    "- `collect()` : renvoie **tous les éléments** au driver (à éviter sur de très gros RDD).\n",
    "- `count()` : compte le nombre d’éléments.\n",
    "- `take(n)` : renvoie les `n` premiers éléments.\n",
    "- `reduce(f)` : réduit les éléments via une fonction associative (`+`, `max`, etc.).\n",
    "- `saveAsTextFile(path)` : écrit le contenu dans des fichiers texte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de814e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"collect() :\", rdd.collect())\n",
    "print(\"count()   :\", rdd.count())\n",
    "print(\"reduce()  :\", rdd.reduce(add))\n",
    "print(\"take(3)   :\", rdd.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d53a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le chemin doit être un dossier qui n'existe pas encore.\n",
    "# Exemple : \"output_rdd\" (Spark va créer le dossier et des fichiers part-* dedans).\n",
    "\n",
    "rdd = sc.parallelize([\"ligne 1\", \"ligne 2\", \"ligne 3\"])\n",
    "\n",
    "rdd.saveAsTextFile(\"output_rdd\")\n",
    "\n",
    "print(\"Données écrites dans le dossier 'output_rdd'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4daaf",
   "metadata": {},
   "source": [
    "Spark considère le résultat comme un ensemble de partitions, donc il écrit :\n",
    "un répertoire : output_rdd/ dedans, plusieurs fichiers texte nommés en général :\n",
    "- part-00000\n",
    "- part-00001\n",
    "- ...\n",
    "\n",
    "et souvent un fichier de marqueur : _SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relire les données écrites dans le dossier \"output_rdd\"\n",
    "rdd2 = sc.textFile(\"output_rdd\")\n",
    "print(rdd2.collect())\n",
    "# ['ligne 1', 'ligne 2', 'ligne 3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aae5f7",
   "metadata": {},
   "source": [
    "## 6. Persistance des RDD : `cache()` et `persist()`\n",
    "\n",
    "Par défaut, un RDD **n’est pas persistant**.  \n",
    "À chaque fois qu’on lance une **action**, Spark **recalcule tout l’historique** du RDD depuis la source.\n",
    "\n",
    "Si tu fais :\n",
    "\n",
    "```scala\n",
    "val rdd = sc.textFile(\"bigfile.txt\").map(...)\n",
    "val a = rdd.count()\n",
    "val b = rdd.collect()\n",
    "```\n",
    "\n",
    "Spark relit et retravaille `bigfile.txt` **deux fois**.\n",
    "\n",
    "### Pourquoi persister un RDD ?\n",
    "\n",
    "- Tu vas **réutiliser** le même RDD dans plusieurs actions.\n",
    "- Les transformations pour construire ce RDD sont **coûteuses** (joins, filtres lourds, etc.).\n",
    "- Tu veux **accélérer** les traitements en évitant les recalculs.\n",
    "\n",
    "### Méthodes :\n",
    "\n",
    "- `rdd.cache()`  \n",
    "  - stocke en mémoire (`MEMORY_ONLY`).  \n",
    "  - suffisant dans beaucoup de cas.\n",
    "\n",
    "- `rdd.persist(storageLevel)`  \n",
    "  - permet de choisir le niveau de stockage (mémoire + disque, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data.txt\").map(lambda line: line.upper())\n",
    "\n",
    "# Première action\n",
    "nb_lignes = rdd.count()\n",
    "\n",
    "# Deuxième action\n",
    "contenu = rdd.collect()\n",
    "\n",
    "print(\"Nombre de lignes :\", nb_lignes)\n",
    "print(\"Premières lignes :\", contenu[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b6b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data.txt\").map(lambda line: line.upper())\n",
    "\n",
    "# Demande à Spark de mettre ce RDD en cache lors de la première action\n",
    "rdd_cached = rdd.cache()\n",
    "\n",
    "# 1ère action : déclenche le calcul + le cache\n",
    "nb_lignes = rdd_cached.count()\n",
    "\n",
    "# 2ème action : Spark lit depuis le cache (plus rapide)\n",
    "contenu = rdd_cached.collect()\n",
    "\n",
    "print(\"Nombre de lignes :\", nb_lignes)\n",
    "print(\"Premières lignes :\", contenu[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6090e9",
   "metadata": {},
   "source": [
    "## 7. Exemple complet : Word Count avec RDD\n",
    "\n",
    "Objectif : trouver les **mots les plus fréquents** dans un fichier texte.\n",
    "\n",
    "Étapes :\n",
    "\n",
    "1. Lire le fichier avec `sc.textFile`.\n",
    "2. Découper chacune des lignes en mots (`flatMap`).\n",
    "3. Associer à chaque mot la valeur 1 (`map` → `(mot, 1)`).\n",
    "4. Agréger par mot (`reduceByKey`).\n",
    "5. Trier par fréquence (`sortBy`).\n",
    "6. Afficher les `N` mots les plus fréquents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Lecture du fichier\n",
    "lines = sc.textFile(\"data.txt\")\n",
    "\n",
    "# 2. Découpage en mots (on peut améliorer le split selon les cas)\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "\n",
    "# 3. (mot) -> (mot, 1)\n",
    "pairs = words.map(lambda w: (w, 1))\n",
    "\n",
    "# 4. Agrégation par mot\n",
    "word_counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# 5. Tri par fréquence décroissante\n",
    "sorted_counts = word_counts.sortBy(lambda kv: kv[1], ascending=False)\n",
    "\n",
    "# 6. Afficher les 20 mots les plus fréquents\n",
    "for word, count in sorted_counts.take(20):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36be6a0",
   "metadata": {},
   "source": [
    "## 8. Conclusion & pistes pour aller plus loin\n",
    "\n",
    "Dans ce notebook, tu as vu :\n",
    "\n",
    "- Ce qu’est un **RDD** (Resilient Distributed Dataset).\n",
    "- Les propriétés clés : **résilience**, **distribution**, **immutabilité**, **lazy evaluation**.\n",
    "- Comment **créer** des RDD (`parallelize`, `textFile`).\n",
    "- Les **transformations** courantes : `map`, `filter`, `flatMap`, `distinct`,\n",
    "  `union`, `intersection`, `groupBy`, `groupByKey`, `reduceByKey`, `sortBy`.\n",
    "- Les **actions** : `collect`, `count`, `take`, `reduce`, `saveAsTextFile`.\n",
    "- La **persistance** avec `cache()` / `persist()`.\n",
    "- Un **workflow complet** RDD avec l'exemple *Word Count*.\n",
    "\n",
    "\n",
    "Pour aller plus loin :\n",
    "\n",
    "- Tester d’autres transformations (`join`, `leftOuterJoin`, etc.).\n",
    "- Comparer RDD et **DataFrames** pour le même problème.\n",
    "- Surveiller l’exécution avec l’**interface Web** de Spark (Spark UI).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6650c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brief-pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
