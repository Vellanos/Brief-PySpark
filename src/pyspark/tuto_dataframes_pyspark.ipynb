{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c59019",
   "metadata": {},
   "source": [
    "# DataFrames avec PySpark : comparaison avec les RDD\n",
    "\n",
    "Ce notebook complète le **tuto RDD** en présentant les **DataFrames** dans PySpark, et en comparant les deux approches sur un même exemple (*Word Count*).\n",
    "\n",
    "Objectifs :\n",
    "- Comprendre la différence **RDD vs DataFrame**.\n",
    "- Créer des **DataFrames** à partir de collections locales et de fichiers.\n",
    "- Manipuler les DataFrames : `select`, `filter`, `withColumn`, `groupBy`, `agg`, `orderBy`.\n",
    "- Implémenter un **Word Count** avec DataFrames.\n",
    "- Comparer Word Count **RDD vs DataFrame** (lisibilité, optimisations, API).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c035a620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/19 05:25:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/19 05:25:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/19 05:25:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x112d679d0>\n",
      "<SparkContext master=local[*] appName=TutoDataFrame_PySpark>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Création d'une SparkSession locale\n",
    "spark = SparkSession.builder.appName(\"TutoDataFrame_PySpark\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# SparkContext, pour la partie comparaison RDD\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c50488",
   "metadata": {},
   "source": [
    "## 1. DataFrame vs RDD : rappel conceptuel\n",
    "\n",
    "Un **RDD** est :\n",
    "\n",
    "- une collection distribuée d'objets Python,\n",
    "- sans schéma explicite,\n",
    "- manipulée avec des fonctions Python (lambda) côté driver,\n",
    "- peu optimisable automatiquement.\n",
    "\n",
    "Un **DataFrame** est :\n",
    "\n",
    "- une **table distribuée** avec un **schéma** (colonnes typées),\n",
    "- manipulée avec une API déclarative (semblable à SQL),\n",
    "- traduite en un **plan logique** puis optimisée par le **Catalyst optimizer** de Spark,\n",
    "- exécutée avec des optimisations bas niveau (projet Tungsten).\n",
    "\n",
    "En pratique, en PySpark moderne :\n",
    "\n",
    "- on privilégie les **DataFrames** (et Spark SQL) pour la plupart des traitements,\n",
    "- on réserve les **RDD** à des cas très spécifiques (API bas niveau, transformations très personnalisées, compatibilité ancienne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8b6d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD : [1, 2, 3, 4, 5]\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = list(range(1, 6))\n",
    "\n",
    "#  RDD\n",
    "rdd_numbers = sc.parallelize(data)\n",
    "\n",
    "# DataFrame\n",
    "df_numbers = spark.createDataFrame([(x,) for x in data], [\"value\"])\n",
    "\n",
    "print(\"RDD :\", rdd_numbers.collect())\n",
    "df_numbers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2e604",
   "metadata": {},
   "source": [
    "## 2. Créer des DataFrames\n",
    "\n",
    "On peut créer un DataFrame de plusieurs façons :\n",
    "\n",
    "1. **À partir d'une collection locale** (liste de tuples, liste de dictionnaires, etc.) avec `spark.createDataFrame(...)`.\n",
    "2. **À partir d'un fichier** avec les lecteurs intégrés : `spark.read.csv`, `spark.read.json`, `spark.read.parquet`, `spark.read.table`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87550b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 25|  1|  Alice|\n",
      "| 30|  2|    Bob|\n",
      "| 35|  3|Charlie|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1. À partir d'une liste de dictionnaires\n",
    "\n",
    "data_people = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 25},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 30},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35},\n",
    "]\n",
    "\n",
    "df_people = spark.createDataFrame(data_people)\n",
    "df_people.printSchema()\n",
    "df_people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f58e25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      "\n",
      "+---+-----+-------+--------+----------+\n",
      "| id| city|product|quantity|unit_price|\n",
      "+---+-----+-------+--------+----------+\n",
      "|  1|Paris|      A|      10|     100.0|\n",
      "|  2| Lyon|      A|       5|     100.0|\n",
      "|  3|Paris|      B|       8|      80.0|\n",
      "+---+-----+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.2. À partir d'une liste de tuples + noms de colonnes\n",
    "\n",
    "data_sales = [\n",
    "    (1, \"Paris\", \"A\", 10, 100.0),\n",
    "    (2, \"Lyon\", \"A\", 5, 100.0),\n",
    "    (3, \"Paris\", \"B\", 8, 80.0),\n",
    "]\n",
    "\n",
    "cols = [\"id\", \"city\", \"product\", \"quantity\", \"unit_price\"]\n",
    "\n",
    "df_sales = spark.createDataFrame(data_sales, cols)\n",
    "df_sales.printSchema()\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134f4bb",
   "metadata": {},
   "source": [
    "### 2.3. Lecture d'un fichier CSV\n",
    "\n",
    "Exemple de lecture d'un fichier CSV (à adapter à ton contexte) :\n",
    "\n",
    "```python\n",
    "df_csv = spark.read.csv(\"ventes.csv\", header=True, inferSchema=True)\n",
    "df_csv.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90f6df",
   "metadata": {},
   "source": [
    "## 3. Opérations de base sur les DataFrames\n",
    "\n",
    "Quelques opérations classiques :\n",
    "\n",
    "- `select` : sélectionner des colonnes.\n",
    "- `filter` / `where` : filtrer les lignes.\n",
    "- `withColumn` : ajouter / transformer une colonne.\n",
    "- `groupBy(...).agg(...)` : agrégations par groupe.\n",
    "- `orderBy` / `sort` : trier le résultat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351ccc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+--------+----------+\n",
      "| id| city|product|quantity|unit_price|\n",
      "+---+-----+-------+--------+----------+\n",
      "|  1|Paris|      A|      10|     100.0|\n",
      "|  2| Lyon|      A|       5|     100.0|\n",
      "|  3|Paris|      B|       8|      80.0|\n",
      "+---+-----+-------+--------+----------+\n",
      "\n",
      "+-----+-------+--------+\n",
      "| city|product|quantity|\n",
      "+-----+-------+--------+\n",
      "|Paris|      A|      10|\n",
      "| Lyon|      A|       5|\n",
      "|Paris|      B|       8|\n",
      "+-----+-------+--------+\n",
      "\n",
      "+---+-----+-------+--------+----------+\n",
      "| id| city|product|quantity|unit_price|\n",
      "+---+-----+-------+--------+----------+\n",
      "|  1|Paris|      A|      10|     100.0|\n",
      "|  3|Paris|      B|       8|      80.0|\n",
      "+---+-----+-------+--------+----------+\n",
      "\n",
      "+---+-----+-------+--------+----------+-------+\n",
      "| id| city|product|quantity|unit_price|revenue|\n",
      "+---+-----+-------+--------+----------+-------+\n",
      "|  1|Paris|      A|      10|     100.0| 1000.0|\n",
      "|  2| Lyon|      A|       5|     100.0|  500.0|\n",
      "|  3|Paris|      B|       8|      80.0|  640.0|\n",
      "+---+-----+-------+--------+----------+-------+\n",
      "\n",
      "+-----+-------------+\n",
      "| city|total_revenue|\n",
      "+-----+-------------+\n",
      "|Paris|       1640.0|\n",
      "| Lyon|        500.0|\n",
      "+-----+-------------+\n",
      "\n",
      "+-----+-------------+\n",
      "| city|total_revenue|\n",
      "+-----+-------------+\n",
      "|Paris|       1640.0|\n",
      "| Lyon|        500.0|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "df_sales.show()\n",
    "\n",
    "# SELECT city, product, quantity\n",
    "df_sel = df_sales.select(\"city\", \"product\", \"quantity\")\n",
    "df_sel.show()\n",
    "\n",
    "# WHERE quantity > 6\n",
    "df_filtered = df_sales.filter(col(\"quantity\") > 6)\n",
    "df_filtered.show()\n",
    "\n",
    "# withColumn : ajouter une colonne 'revenue' = quantity * unit_price\n",
    "df_revenue = df_sales.withColumn(\"revenue\", col(\"quantity\") * col(\"unit_price\"))\n",
    "df_revenue.show()\n",
    "\n",
    "# groupBy city : somme du revenue par ville\n",
    "df_city_revenue = df_revenue.groupBy(\"city\").agg(expr(\"sum(revenue) as total_revenue\"))\n",
    "df_city_revenue.show()\n",
    "\n",
    "# Trier par revenue décroissant\n",
    "df_city_revenue.orderBy(col(\"total_revenue\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035556ab",
   "metadata": {},
   "source": [
    "## 4. Transformations DataFrame vs RDD\n",
    "\n",
    "Quelques parallèles :\n",
    "\n",
    "- **RDD** : `map(lambda x: ...)`  \n",
    "  **DF**  : `select(expr(...))`, `withColumn(...)`\n",
    "\n",
    "- **RDD** : `filter(lambda x: condition)`  \n",
    "  **DF**  : `filter(condition)` ou `where(condition)`\n",
    "\n",
    "- **RDD** : RDD de paires `(key, value)` + `reduceByKey`  \n",
    "  **DF**  : `groupBy(\"key\").agg(...)`\n",
    "\n",
    "- **RDD** : `sortBy(...)`  \n",
    "  **DF**  : `orderBy(...)` / `sort(...)`\n",
    "\n",
    "L'idée générale :  \n",
    "> Avec les DataFrames, on écrit **ce qu'on veut obtenir**,  \n",
    "> Spark se charge de **comment l'exécuter efficacement** (optimiseur Catalyst)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3909f582",
   "metadata": {},
   "source": [
    "## 5. Word Count avec DataFrames\n",
    "\n",
    "On va maintenant implémenter **Word Count** avec DataFrames, en réutilisant le même fichier texte `data.txt` que dans le notebook RDD.\n",
    "\n",
    "### Étapes (version DataFrame) :\n",
    "\n",
    "1. Lire le fichier texte avec `spark.read.text(\"data.txt\")` → une colonne `value`.\n",
    "2. Nettoyer le texte (optionnel mais utile) : mettre en minuscule, enlever la ponctuation.\n",
    "3. Découper la colonne `value` en mots (`split`), puis exploser en lignes (`explode`).\n",
    "4. Filtrer les mots vides.\n",
    "5. `groupBy(\"word\").count()`.\n",
    "6. Trier par `count` décroissant et afficher les N premiers mots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fa44ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------+\n",
      "|value                                                                                   |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "|Spark est un moteur de calcul distribué pour le big data.                               |\n",
      "|Le big data désigne des volumes de données trop importants pour un traitement classique.|\n",
      "|Avec Spark, on peut traiter des données en mémoire de manière très efficace.            |\n",
      "|Un RDD est une collection distribuée, immuable et tolérante aux pannes.                 |\n",
      "|Les DataFrames offrent une API de plus haut niveau que les RDD.                         |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, lower, regexp_replace, desc\n",
    "\n",
    "# 1. Lecture du fichier texte\n",
    "# Adapte le chemin à ton environnement, comme dans le notebook RDD\n",
    "df_lines = spark.read.text(\"data.txt\")\n",
    "\n",
    "df_lines.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327b2fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------+\n",
      "|line                                                                                    |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "|spark est un moteur de calcul distribu  pour le big data                                |\n",
      "|le big data d signe des volumes de donn es trop importants pour un traitement classique |\n",
      "|avec spark  on peut traiter des donn es en m moire de mani re tr s efficace             |\n",
      "|un rdd est une collection distribu e  immuable et tol rante aux pannes                  |\n",
      "|les dataframes offrent une api de plus haut niveau que les rdd                          |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Nettoyage de base : \n",
    "# - mise en minuscule\n",
    "# - suppression des caractères de ponctuation (remplacés par un espace)\n",
    "\n",
    "df_clean = df_lines.select(\n",
    "    lower(\n",
    "        regexp_replace(col(\"value\"), r\"[^\\w\\s]\", \" \")\n",
    "    ).alias(\"line\")\n",
    ")\n",
    "\n",
    "df_clean.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86248af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    word|\n",
      "+--------+\n",
      "|   spark|\n",
      "|     est|\n",
      "|      un|\n",
      "|  moteur|\n",
      "|      de|\n",
      "|  calcul|\n",
      "|distribu|\n",
      "|    pour|\n",
      "|      le|\n",
      "|     big|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Split + explode pour obtenir une ligne par mot\n",
    "\n",
    "df_words = df_clean.select(\n",
    "    explode(\n",
    "        split(col(\"line\"), \"\\s+\")\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "\n",
    "df_words.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d4e779b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  les|   30|\n",
      "|   de|   21|\n",
      "|   le|   18|\n",
      "|    d|   14|\n",
      "| pour|   13|\n",
      "|spark|   13|\n",
      "|   et|   13|\n",
      "|    r|   12|\n",
      "|   un|   12|\n",
      "|  des|   11|\n",
      "|   es|   10|\n",
      "| avec|   10|\n",
      "|  rdd|    9|\n",
      "| peut|    8|\n",
      "| dans|    8|\n",
      "|count|    7|\n",
      "|   en|    7|\n",
      "| sont|    7|\n",
      "|    s|    7|\n",
      "|   ou|    7|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Filtrer les chaînes vides\n",
    "df_words_non_empty = df_words.filter(col(\"word\") != \"\")\n",
    "\n",
    "# 5. groupBy + count\n",
    "df_word_counts = df_words_non_empty.groupBy(\"word\").count()\n",
    "\n",
    "# 6. Tri décroissant sur count\n",
    "df_top_words = df_word_counts.orderBy(desc(\"count\"))\n",
    "\n",
    "df_top_words.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16fe39",
   "metadata": {},
   "source": [
    "## 6. Comparaison avec Word Count en RDD\n",
    "\n",
    "Rappel : la version RDD du Word Count ressemble à ceci :\n",
    "\n",
    "```python\n",
    "lines = sc.textFile(\"data.txt\")\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "pairs = words.map(lambda w: (w, 1))\n",
    "word_counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "sorted_counts = word_counts.sortBy(lambda kv: kv[1], ascending=False)\n",
    "sorted_counts.take(20)\n",
    "```\n",
    "\n",
    "On va réexécuter une version RDD ici pour comparer le code et les résultats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322befe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 21),\n",
       " ('les', 19),\n",
       " ('le', 14),\n",
       " ('et', 13),\n",
       " ('pour', 13),\n",
       " ('des', 11),\n",
       " ('Les', 11),\n",
       " ('avec', 9),\n",
       " ('un', 8),\n",
       " ('peut', 8),\n",
       " ('RDD', 8),\n",
       " ('sont', 7),\n",
       " ('Spark', 7),\n",
       " ('en', 7),\n",
       " ('ou', 7),\n",
       " ('est', 6),\n",
       " ('count', 6),\n",
       " ('sur', 5),\n",
       " ('fichiers', 5),\n",
       " ('données', 5)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Count version RDD \n",
    "\n",
    "lines_rdd = sc.textFile(\"data.txt\")\n",
    "words_rdd = lines_rdd.flatMap(lambda line: line.split())\n",
    "\n",
    "pairs_rdd = words_rdd.map(lambda w: (w, 1))\n",
    "word_counts_rdd = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "sorted_counts_rdd = word_counts_rdd.sortBy(lambda kv: kv[1], ascending=False)\n",
    "\n",
    "top20_rdd = sorted_counts_rdd.take(20)\n",
    "top20_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df92d223",
   "metadata": {},
   "source": [
    "## 7. Discussion : RDD vs DataFrame sur Word Count\n",
    "\n",
    "### 7.1 Lisibilité du code\n",
    "\n",
    "- **RDD** : on manipule des tuples `(mot, 1)` et des lambdas Python.\n",
    "- **DataFrame** : on manipule des colonnes nommées (`word`, `count`) avec des opérations de type SQL (`groupBy`, `count`, `orderBy`).\n",
    "\n",
    "La version DataFrame est souvent plus **déclarative** et plus simple à lire pour quelqu'un habitué à SQL / BI.\n",
    "\n",
    "### 7.2 Optimisations\n",
    "\n",
    "- Les DataFrames passent par l'**optimiseur Catalyst**.\n",
    "- Spark peut **réorganiser le plan**, **pousser les filtres**, **optimiser les agrégations**, etc.\n",
    "- Avec des RDD, Spark voit seulement des fonctions Python opaques → **moins d'optimisations automatiques**.\n",
    "\n",
    "### 7.3 Typage et schéma\n",
    "\n",
    "- RDD : pas de schéma global, juste des objets Python.\n",
    "- DataFrame : schéma explicite, facilité d'intégration avec des outils SQL (Spark SQL, JDBC, BI, etc.).\n",
    "\n",
    "### 7.4 Quand utiliser quoi ?\n",
    "\n",
    "- **DataFrames** (recommandé dans la majorité des cas) :\n",
    "  - traitements de données structurées / semi-structurées,\n",
    "  - jointures, agrégations, pipelines de transformations,\n",
    "  - intégration avec Spark SQL, MLlib (DataFrame API), etc.\n",
    "\n",
    "- **RDD** :\n",
    "  - besoin de transformations très bas niveau / personnalisées,\n",
    "  - besoin d'une API fonctionnelle proche de l'historique de Spark,\n",
    "  - cas spécifiques non bien couverts par l'API DataFrame.\n",
    "\n",
    "En PySpark moderne, la bonne pratique est :  \n",
    "> Commencer par **DataFrames**, et ne descendre vers les **RDD** que si nécessaire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd3f703",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "Dans ce notebook, tu as :\n",
    "\n",
    "- créé des **DataFrames** à partir de listes Python et de texte,\n",
    "- manipulé des DataFrames avec `select`, `filter`, `withColumn`, `groupBy`, `orderBy`,\n",
    "- implémenté un **Word Count** avec DataFrames,\n",
    "- comparé cette approche avec la version **RDD**.\n",
    "\n",
    "N'oublie pas d'arrêter la session Spark une fois terminé :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14beb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
